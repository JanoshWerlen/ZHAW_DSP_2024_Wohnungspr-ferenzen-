{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Stations form SBB API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os\n",
    "import sqlite3\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database file\n",
    "DATABASE_FILE = \"locations.db\"\n",
    "\n",
    "# Function to initialize the database\n",
    "def initialize_database():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create table for locations if it doesn't exist\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS locations (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            name TEXT,\n",
    "            coordinate_x REAL,\n",
    "            coordinate_y REAL,\n",
    "            distance REAL\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Function to check if a city already has data in the database\n",
    "def city_exists_in_db(city):\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT 1 FROM locations WHERE name = ? LIMIT 1\n",
    "        \"\"\",\n",
    "        (city,)\n",
    "    )\n",
    "    exists = cursor.fetchone() is not None\n",
    "    conn.close()\n",
    "    return exists\n",
    "\n",
    "# Function to insert data into the database\n",
    "def insert_into_database(data):\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.executemany(\n",
    "        \"\"\"\n",
    "        INSERT OR IGNORE INTO locations (id, name, coordinate_x, coordinate_y, distance)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        data,\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Function to fetch location data for multiple cities\n",
    "def fetch_locations(cities):\n",
    "    \"\"\"Fetches location data for a list of cities and saves them to the database.\"\"\"\n",
    "    for idx, city in enumerate(cities, 1):\n",
    "        clear_output(wait=True)  # Clear the output\n",
    "        print(f\"Processing city {idx}/{len(cities)}: {city}\")\n",
    "\n",
    "        if city_exists_in_db(city):\n",
    "            print(f\"Data for {city} already exists in the database. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        url = f\"http://transport.opendata.ch/v1/locations?query={city}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                stations = data.get(\"stations\", [])\n",
    "\n",
    "                # Prepare data for database insertion\n",
    "                db_data = [\n",
    "                    (\n",
    "                        station.get(\"id\", \"\"),\n",
    "                        station.get(\"name\", \"\"),\n",
    "                        station.get(\"coordinate\", {}).get(\"x\", None),\n",
    "                        station.get(\"coordinate\", {}).get(\"y\", None),\n",
    "                        station.get(\"distance\", None),\n",
    "                    )\n",
    "                    for station in stations\n",
    "                ]\n",
    "\n",
    "                insert_into_database(db_data)\n",
    "            else:\n",
    "                print(f\"Error fetching data for {city}: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for {city}: {e}\")\n",
    "\n",
    "# Function to read data from the database for analysis\n",
    "def fetch_data_from_db():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    query = \"SELECT * FROM locations\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize database\n",
    "    initialize_database()\n",
    "\n",
    "    # List of Swiss cities near Zurich\n",
    "     \n",
    "    cities_near_zurich = [\n",
    "    # Zurich neighborhoods\n",
    "    \"Zurich\", \"Altstetten\", \"Albisrieden\", \"Oberstrass\", \"Unterstrass\", \"Seebach\", \"Affoltern\", \"Höngg\",\n",
    "    \"Wipkingen\", \"Wiedikon\", \"Enge\", \"Wollishofen\", \"Leimbach\", \"Fluntern\", \"Hottingen\", \"Hirslanden\", \"Mühlebach\",\n",
    "\n",
    "    # Municipalities in Zurich Canton\n",
    "    \"Adlikon\", \"Adliswil\", \"Aesch\", \"Aeugst am Albis\", \"Affoltern am Albis\", \"Altikon\", \"Andelfingen\",\n",
    "    \"Bachenbülach\", \"Bachs\", \"Bäretswil\", \"Bassersdorf\", \"Bauma\", \"Benken\", \"Berg am Irchel\", \"Birmensdorf\",\n",
    "    \"Bonstetten\", \"Boppelsen\", \"Brütten\", \"Bubikon\", \"Buch am Irchel\", \"Buchs\", \"Bülach\", \"Dachsen\", \"Dägerlen\",\n",
    "    \"Dällikon\", \"Dänikon\", \"Dättlikon\", \"Dielsdorf\", \"Dietikon\", \"Dietlikon\", \"Dinhard\", \"Dorf\", \"Dübendorf\",\n",
    "    \"Dürnten\", \"Egg\", \"Eglisau\", \"Elgg\", \"Ellikon an der Thur\", \"Elsau\", \"Embrach\", \"Erlenbach\", \"Fällanden\",\n",
    "    \"Fehraltorf\", \"Feuerthalen\", \"Fischenthal\", \"Flaach\", \"Flurlingen\", \"Freienstein-Teufen\", \"Geroldswil\",\n",
    "    \"Glattfelden\", \"Gossau\", \"Greifensee\", \"Grüningen\", \"Hagenbuch\", \"Hausen am Albis\", \"Hedingen\", \"Henggart\",\n",
    "    \"Herrliberg\", \"Hettlingen\", \"Hinwil\", \"Hittnau\", \"Hochfelden\", \"Hombrechtikon\", \"Horgen\", \"Höri\", \"Hüntwangen\",\n",
    "    \"Hüttikon\", \"Illnau-Effretikon\", \"Kappel am Albis\", \"Kilchberg\", \"Kleinandelfingen\", \"Kloten\", \"Knonau\",\n",
    "    \"Küsnacht\", \"Langnau am Albis\", \"Laufen-Uhwiesen\", \"Lindau\", \"Lufingen\", \"Männedorf\", \"Marthalen\", \"Maschwanden\",\n",
    "    \"Maur\", \"Meilen\", \"Mettmenstetten\", \"Mönchaltorf\", \"Neerach\", \"Neftenbach\", \"Niederglatt\", \"Niederhasli\",\n",
    "    \"Niederweningen\", \"Nürensdorf\", \"Oberembrach\", \"Oberengstringen\", \"Oberglatt\", \"Oberrieden\", \"Oberweningen\",\n",
    "    \"Obfelden\", \"Oetwil am See\", \"Oetwil an der Limmat\", \"Opfikon\", \"Ossingen\", \"Otelfingen\", \"Ottenbach\", \"Pfäffikon\",\n",
    "    \"Pfungen\", \"Rafz\", \"Regensberg\", \"Regensdorf\", \"Rheinau\", \"Richterswil\", \"Rickenbach\", \"Rifferswil\", \"Rikon im Tösstal\",\n",
    "    \"Rorbas\", \"Rümlang\", \"Rüschlikon\", \"Russikon\", \"Rüti\", \"Schlatt\", \"Schleinikon\", \"Schlieren\", \"Schöfflisdorf\",\n",
    "    \"Schwerzenbach\", \"Seegräben\", \"Seuzach\", \"Stadel\", \"Stäfa\", \"Stallikon\", \"Stammheim\", \"Steinmaur\", \"Thalheim an der Thur\",\n",
    "    \"Thalwil\", \"Trüllikon\", \"Truttikon\", \"Turbenthal\", \"Uetikon am See\", \"Uitikon\", \"Unterengstringen\", \"Urdorf\", \"Uster\",\n",
    "    \"Volken\", \"Volketswil\", \"Wädenswil\", \"Wald\", \"Wallisellen\", \"Wangen-Brüttisellen\", \"Wasterkingen\", \"Weiach\",\n",
    "    \"Weiningen\", \"Weisslingen\", \"Wettswil am Albis\", \"Wetzikon\", \"Wiesendangen\", \"Wil\", \"Wila\", \"Wildberg\", \"Winkel\",\n",
    "    \"Winterthur\", \"Zell\", \"Zollikon\", \"Zumikon\", \n",
    "\n",
    "    # Surrounding regions (Aargau, Zug, Schwyz, etc.)\n",
    "    \"Bremgarten\", \"Spreitenbach\", \"Wohlen\", \"Wettingen\", \"Baar\", \"Cham\", \"Jona\", \"Illnau\", \"Schindellegi\",\n",
    "    \"Oberägeri\", \"Unterägeri\", \"Freienbach\", \"Pfäffikon SZ\", \"Rapperswil-Jona\", \"Tuggen\", \"Wollerau\", \"Zug\",\n",
    "    \"Horgenberg\", \"Schwyz\", \"Richterswil\", \"Sattel\", \"Arth\", \"Goldau\", \"Steinhausen\", \"Cham\", \"Rotkreuz\",\n",
    "    \n",
    "    # Other villages and named areas\n",
    "    \"Adlisberg\", \"Gockhausen\", \"Stettbach\", \"Itschnach\", \"Hinteregg\", \"Forch\", \"Schönenberg\", \"Hütten\", \"Küsnachterberg\",\n",
    "    \"Wängi\", \"Berikon\", \"Dietwil\", \"Einsiedeln\", \"Biberbrugg\", \"Wädenswilberg\", \"Thalwilberg\", \"Frauenfeld\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Fetch and save location data\n",
    "    fetch_locations(cities_near_zurich)\n",
    "\n",
    "    # Fetch data for analysis\n",
    "    print(\"Fetching data for analysis...\")\n",
    "    df = fetch_data_from_db()\n",
    "    print(df.head())  # Display the first few rows of the data\n",
    "\n",
    "    print(\"Data fetching and saving completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Database file\n",
    "DATABASE_FILE = \"locations.db\"\n",
    "\n",
    "# Function to clean and transform data in the database\n",
    "def clean_and_transform_data():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    \n",
    "    # Load the data into a pandas DataFrame\n",
    "    query = \"SELECT * FROM locations\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "\n",
    "    # Remove rows where the 'id' column is empty\n",
    "    df = df.dropna(subset=['id'])\n",
    "\n",
    "    # Split the 'name' column into two parts at the comma\n",
    "    df[['name_part_1', 'name_part_2']] = df['name'].str.split(',', n=1, expand=True)\n",
    "\n",
    "    # Replace NaN in 'name_part_2' with an empty string\n",
    "    df['name_part_2'] = df['name_part_2'].fillna('')\n",
    "\n",
    "    # Drop the original 'name' column if not needed\n",
    "    df = df.drop(columns=['name'])\n",
    "\n",
    "    # Save the cleaned and transformed data back into the database (optional table)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS cleaned_locations\")\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE cleaned_locations (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            coordinate_x REAL,\n",
    "            coordinate_y REAL,\n",
    "            distance REAL,\n",
    "            name_part_1 TEXT,\n",
    "            name_part_2 TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Insert cleaned data back into the database\n",
    "    df.to_sql('cleaned_locations', conn, if_exists='replace', index=False)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Cleaned data has been saved to the 'cleaned_locations' table in the database.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_and_transform_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Database file\n",
    "DATABASE_FILE = \"locations.db\"\n",
    "\n",
    "def combine_data():\n",
    "    try:\n",
    "        conn = sqlite3.connect(DATABASE_FILE)\n",
    "\n",
    "        # Load the cleaned data into a pandas DataFrame\n",
    "        query = \"SELECT * FROM cleaned_locations\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "\n",
    "        if df.empty:\n",
    "            print(\"No data found in 'cleaned_locations' table.\")\n",
    "            return\n",
    "\n",
    "        # Define a function to select the row with the shortest name_part_2\n",
    "        def select_shortest_name_part_2(group):\n",
    "            return group.loc[group['name_part_2'].str.len().idxmin()]\n",
    "\n",
    "        # Group by 'name_part_1' and apply the function\n",
    "        combined_df = (\n",
    "            df.groupby('name_part_1', group_keys=False)\n",
    "            .apply(select_shortest_name_part_2)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "\n",
    "        # Create a new table for combined data\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS combined_locations\")\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            CREATE TABLE combined_locations (\n",
    "                id TEXT PRIMARY KEY,\n",
    "                name_part_1 TEXT,\n",
    "                name_part_2 TEXT,\n",
    "                coordinate_x REAL,\n",
    "                coordinate_y REAL,\n",
    "                distance REAL\n",
    "            )\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # Insert combined data back into the database\n",
    "        combined_df.to_sql('combined_locations', conn, if_exists='replace', index=False)\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print(\"Combined data has been saved to the 'combined_locations' table in the database.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    combine_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "\n",
    "# Database file\n",
    "DATABASE_FILE = \"locations.db\"\n",
    "\n",
    "# Initialize the transport results table\n",
    "def initialize_transport_table():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS transport_results_2 (\n",
    "            from_station TEXT,\n",
    "            to_station TEXT,\n",
    "            min_duration INTEGER,\n",
    "            max_duration INTEGER,\n",
    "            transfers INTEGER,\n",
    "            connections INTEGER,\n",
    "            legs TEXT,\n",
    "            PRIMARY KEY (from_station, to_station)\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Check if transport data exists in the database\n",
    "def transport_data_exists(from_station, to_station):\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT 1 FROM transport_results_2 WHERE from_station = ? AND to_station = ? LIMIT 1\n",
    "        \"\"\",\n",
    "        (from_station, to_station),\n",
    "    )\n",
    "    exists = cursor.fetchone() is not None\n",
    "    conn.close()\n",
    "    return exists\n",
    "\n",
    "# Save a single transport result to the database\n",
    "def save_single_transport_data(from_station, to_station, min_duration, max_duration, transfers, connections, legs):\n",
    "    # Convert duration from seconds to minutes\n",
    "    min_duration_in_minutes = min_duration // 60  # Integer division to avoid fractions\n",
    "    max_duration_in_minutes = max_duration // 60\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT OR REPLACE INTO transport_results_2 \n",
    "        (from_station, to_station, min_duration, max_duration, transfers, connections, legs)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (from_station, to_station, min_duration_in_minutes, max_duration_in_minutes, transfers, connections, json.dumps(legs)),  # Added legs\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Process transport data with API integration\n",
    "def process_transport_data(to_station, delay=1.5, travel_date=\"12/17/2024\", travel_time=\"08:00\", num=4):\n",
    "    try:\n",
    "        # Fetch starting stations\n",
    "        conn = sqlite3.connect(DATABASE_FILE)\n",
    "        query = \"SELECT DISTINCT name_part_1 FROM combined_locations\"\n",
    "        from_stations = pd.read_sql_query(query, conn)['name_part_1'].dropna().tolist()\n",
    "        conn.close()\n",
    "\n",
    "        if not from_stations:\n",
    "            print(\"No starting stations found in the database.\")\n",
    "            return\n",
    "\n",
    "        session = requests.Session()\n",
    "\n",
    "        for index, from_station in enumerate(from_stations, start=1):\n",
    "            \n",
    "            print(f\"Processing {index}/{len(from_stations)}: From '{from_station}' to '{to_station}'...\")\n",
    "\n",
    "            # Skip if the data already exists\n",
    "            if transport_data_exists(from_station, to_station):\n",
    "                print(f\"Data already exists for '{from_station}' to '{to_station}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Simulate delay to respect API rate limits\n",
    "            #time.sleep(random.uniform(delay, delay + 1))\n",
    "\n",
    "            # API request\n",
    "            params = {\n",
    "                \"from\": from_station,\n",
    "                \"to\": to_station,\n",
    "                \"date\": travel_date,\n",
    "                \"time\": travel_time,\n",
    "                \"num\": num,\n",
    "            }\n",
    "            url = \"https://search.ch/timetable/api/route.json\"\n",
    "            response = session.get(url, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data.get('connections'):\n",
    "                    # Extract details from the response\n",
    "                    min_duration = data.get(\"min_duration\", 0)\n",
    "                    max_duration = data.get(\"max_duration\", 0)\n",
    "                    connections = len(data.get(\"connections\", []))\n",
    "                    transfers = len(data[\"connections\"][0].get(\"legs\", [])) - 1\n",
    "\n",
    "                    # Extract detailed legs information\n",
    "                    legs = [\n",
    "                        {\n",
    "                            \"type\": leg.get(\"type\"),\n",
    "                            \"line\": leg.get(\"line\"),\n",
    "                            \"departure\": leg.get(\"departure\"),\n",
    "                            \"arrival\": leg.get(\"arrival\"),\n",
    "                            \"stops\": [\n",
    "                                stop.get(\"name\") for stop in leg.get(\"stops\", [])\n",
    "                            ] if leg.get(\"stops\") else []\n",
    "                        }\n",
    "                        for leg in data[\"connections\"][0].get(\"legs\", [])\n",
    "                    ]\n",
    "\n",
    "                    # Save the data immediately to the database\n",
    "                    save_single_transport_data(from_station, to_station, min_duration, max_duration, transfers, connections, legs)\n",
    "                    print(f\"Saved data for '{from_station}' to '{to_station}'.\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"No connections found for '{from_station}' to '{to_station}'.\")\n",
    "            else:\n",
    "                print(f\"Error fetching data for '{from_station}' to '{to_station}': {response.status_code}\")\n",
    "\n",
    "        print(\"\\nProcessing complete. All data has been saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize transport table\n",
    "    initialize_transport_table()\n",
    "\n",
    "    # Destination station\n",
    "    to_station = \"Winterthur, Hauptbahnhof\"\n",
    "\n",
    "    # Debugging: Check combined_locations table content\n",
    "    print(\"Checking combined_locations table...\")\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    query = \"SELECT DISTINCT name_part_1 FROM combined_locations\"\n",
    "    from_stations = pd.read_sql_query(query, conn)['name_part_1'].dropna().tolist()\n",
    "    conn.close()\n",
    "\n",
    "    if not from_stations:\n",
    "        print(\"No starting stations found in the combined_locations table.\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(from_stations)} starting stations:\")\n",
    "        print(from_stations[:10])  # Print first 10 stations for verification\n",
    "\n",
    "    process_transport_data(to_station)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET PLZ for all Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"API key not found. Please check your .env file and environment setup.\")\n",
    "\n",
    "# Define the LocationIQ Reverse Geocoding API\n",
    "API_URL = \"https://us1.locationiq.com/v1/reverse\"\n",
    "\n",
    "# Database file\n",
    "DATABASE_FILE = \"locations.db\"\n",
    "\n",
    "# Function to initialize the PLZ table\n",
    "def initialize_plz_table():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS plz_data (\n",
    "            coordinate_x REAL,\n",
    "            coordinate_y REAL,\n",
    "            plz TEXT,\n",
    "            PRIMARY KEY (coordinate_x, coordinate_y)\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Function to check if PLZ exists in the database\n",
    "def plz_exists(lat, lon):\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT plz FROM plz_data WHERE coordinate_x = ? AND coordinate_y = ? LIMIT 1\n",
    "        \"\"\",\n",
    "        (lat, lon),\n",
    "    )\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    return result[0] if result else None\n",
    "\n",
    "# Function to save PLZ data to the database\n",
    "def save_plz_data(lat, lon, plz):\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT OR REPLACE INTO plz_data (coordinate_x, coordinate_y, plz)\n",
    "        VALUES (?, ?, ?)\n",
    "        \"\"\",\n",
    "        (lat, lon, plz),\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Function to query the reverse geocoding API\n",
    "def query_reverse_geocoding(lat, lon):\n",
    "    params = {\n",
    "        \"key\": API_KEY,\n",
    "        \"lat\": lat,\n",
    "        \"lon\": lon,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    response = requests.get(API_URL, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        address = result.get(\"address\", {})\n",
    "        return address.get(\"postcode\", \"N/A\")\n",
    "    else:\n",
    "        print(f\"Error for coordinates ({lat}, {lon}): {response.status_code}\")\n",
    "    return \"N/A\"\n",
    "\n",
    "# Function to process the data with progress updates\n",
    "def process_and_query_plz():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    query = \"SELECT * FROM combined_locations\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "\n",
    "    # Check if required columns exist\n",
    "    if not {'coordinate_x', 'coordinate_y'}.issubset(df.columns):\n",
    "        print(\"Error: The database does not contain 'coordinate_x' or 'coordinate_y' columns.\")\n",
    "        return\n",
    "\n",
    "    # Prepare a list to store PLZ values\n",
    "    plz_list = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        clear_output(wait=True)  # Clear output to reduce clutter\n",
    "        lat, lon = row[\"coordinate_x\"], row[\"coordinate_y\"]\n",
    "        print(f\"Processing {index + 1}/{len(df)}: Coordinates ({lat}, {lon})\")\n",
    "\n",
    "        # Check if PLZ already exists in the database\n",
    "        existing_plz = plz_exists(lat, lon)\n",
    "        if existing_plz:\n",
    "            print(f\"PLZ for ({lat}, {lon}) found in database: {existing_plz}\")\n",
    "            plz_list.append(existing_plz)\n",
    "        else:\n",
    "            # Query the reverse geocoding API\n",
    "            plz = query_reverse_geocoding(lat, lon)\n",
    "            save_plz_data(lat, lon, plz)\n",
    "            plz_list.append(plz)\n",
    "\n",
    "            # Wait 1 second between requests to respect rate limits\n",
    "            time.sleep(1)\n",
    "\n",
    "    # Append the PLZ column to the DataFrame\n",
    "    df[\"plz\"] = plz_list\n",
    "\n",
    "    # Save the updated DataFrame back to the database\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    df.to_sql(\"combined_locations_with_plz\", conn, if_exists=\"replace\", index=False)\n",
    "    conn.close()\n",
    "\n",
    "    print(\"\\nPLZ data has been updated and saved to the database.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the PLZ table\n",
    "    initialize_plz_table()\n",
    "\n",
    "    # Process data and query PLZ\n",
    "    process_and_query_plz()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Database file\n",
    "DATABASE_FILE = \"locations.db\"\n",
    "\n",
    "# Initialize the listings table\n",
    "def initialize_listings_table():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS listings (\n",
    "            plz TEXT,\n",
    "            maxprice INTEGER,\n",
    "            RootPropertyType INTEGER,\n",
    "            listing_count INTEGER,\n",
    "            PRIMARY KEY (plz, maxprice, RootPropertyType)\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Initialize the error logs table\n",
    "def initialize_error_logs_table():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS error_logs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            plz TEXT,\n",
    "            maxprice INTEGER,\n",
    "            RootPropertyType INTEGER,\n",
    "            http_code INTEGER,\n",
    "            url TEXT,\n",
    "            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Save error logs to the database\n",
    "def save_error_log(plz, maxprice, RootPropertyType, http_code, url):\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO error_logs (plz, maxprice, RootPropertyType, http_code, url)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (plz, maxprice, RootPropertyType, http_code, url),\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Fetch PLZs from the database\n",
    "def fetch_plz_from_db():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    query = \"SELECT DISTINCT plz FROM plz_data\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df['plz'].tolist()\n",
    "\n",
    "# Check if listing data exists in the database\n",
    "def listing_exists(plz, maxprice, RootPropertyType):\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT listing_count FROM listings \n",
    "        WHERE plz = ? AND maxprice = ? AND RootPropertyType = ? LIMIT 1\n",
    "        \"\"\",\n",
    "        (plz, maxprice, RootPropertyType),\n",
    "    )\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    return result[0] if result else None\n",
    "\n",
    "# Save listing data to the database\n",
    "def save_listing_data(plz, maxprice, RootPropertyType, listing_count):\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        INSERT OR REPLACE INTO listings (plz, maxprice, RootPropertyType, listing_count)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "        \"\"\",\n",
    "        (plz, maxprice, RootPropertyType, listing_count),\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Fetch a webpage with retries and log HTTP errors\n",
    "def fetch_with_retries(url, session, plz, maxprice, RootPropertyType, retries=3, delay=2.5):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "    for attempt in range(retries):\n",
    "        sleep_time = random.uniform(delay, delay + 1)\n",
    "        print(f\"Attempt {attempt + 1}/{retries}: Waiting {sleep_time:.2f} seconds before fetching page...\")\n",
    "        time.sleep(sleep_time)\n",
    "        response = session.get(url, headers=headers)\n",
    "        if response.status_code == 200 and \"Resultate\" in response.text:\n",
    "            return response.text\n",
    "        print(f\"HTTP Error {response.status_code}. Retrying...\")\n",
    "    print(f\"Failed to fetch content after {retries} retries. Logging error.\")\n",
    "    save_error_log(plz, maxprice, RootPropertyType, response.status_code, url)\n",
    "    return None\n",
    "\n",
    "# Scrape the total count of listings from the page content\n",
    "def scrape_page(soup):\n",
    "    result_text = soup.find(string=re.compile(r'(\\d+)\\s+Resultate'))\n",
    "    if result_text:\n",
    "        match = re.search(r'(\\d+)\\s+Resultate', result_text)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    return 0\n",
    "\n",
    "# Process PLZ values and scrape listings for different maxprices and RootPropertyTypes\n",
    "def process_plz_with_retries(maxprice_list, RootPropertyType_list, scrape_limit=20):\n",
    "    try:\n",
    "        # Fetch PLZs from the database\n",
    "        plz_list = fetch_plz_from_db()\n",
    "        if not plz_list:\n",
    "            print(\"No PLZ values found in the database.\")\n",
    "            return\n",
    "\n",
    "        scrape_count = 0\n",
    "        session = requests.Session()  # Start initial session\n",
    "\n",
    "        for maxprice in maxprice_list:\n",
    "            for RootPropertyType in RootPropertyType_list:\n",
    "                for index, plz in enumerate(plz_list):\n",
    "                    clear_output(wait=True)  # Clear output to prevent clutter\n",
    "                    print(f\"Processing PLZ {index + 1}/{len(plz_list)}: {plz} | Max Price: {maxprice} | RootPropertyType: {RootPropertyType}\")\n",
    "\n",
    "                    # Recreate session every `scrape_limit` scrapes\n",
    "                    if scrape_count > 0 and scrape_count % scrape_limit == 0:\n",
    "                        print(\"Refreshing session to avoid detection...\")\n",
    "                        session = requests.Session()\n",
    "\n",
    "                    # Check if the listing count exists in the database\n",
    "                    existing_listing = listing_exists(plz, maxprice, RootPropertyType)\n",
    "                    if existing_listing is not None:\n",
    "                        print(f\"Listing data for PLZ '{plz}', Max Price '{maxprice}', Root Property Type '{RootPropertyType}' already exists: {existing_listing}.\")\n",
    "                        continue\n",
    "\n",
    "                    # Construct the request object and URL\n",
    "                    requestobject = (\n",
    "                        '{\"DealType\":10,\"SiteId\":0,\"RootPropertyTypes\":[' + str(RootPropertyType) + '],\"PropertyTypes\":[],\"RoomsFrom\":null,'\n",
    "                        '\"RoomsTo\":null,\"PriceFrom\":null,\"PriceTo\":' + str(maxprice) + ',\"LocationSearchString\":\"' + plz + '\",'\n",
    "                        '\"Sort\":3}'\n",
    "                    )\n",
    "                    base_url = \"https://www.comparis.ch/immobilien/result/list\"\n",
    "                    query_string = f\"requestobject={urllib.parse.quote(requestobject)}&page=0\"\n",
    "                    url = f\"{base_url}?{query_string}\"\n",
    "\n",
    "                    # Fetch and scrape the page\n",
    "                    page_content = fetch_with_retries(url, session, plz, maxprice, RootPropertyType)\n",
    "                    if page_content:\n",
    "                        soup = BeautifulSoup(page_content, \"html.parser\")\n",
    "                        listing_count = scrape_page(soup)\n",
    "                        print(f\"Found {listing_count} listings for PLZ '{plz}', Max Price '{maxprice}', Root Property Type '{RootPropertyType}'.\")\n",
    "                        save_listing_data(plz, maxprice, RootPropertyType, listing_count)\n",
    "                    else:\n",
    "                        print(f\"Failed to fetch data for PLZ '{plz}'. Debugging the issue...\")\n",
    "\n",
    "                    # Increment scrape count\n",
    "                    scrape_count += 1\n",
    "\n",
    "        print(\"\\nScraping complete. All data saved to the database.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize tables\n",
    "    initialize_listings_table()\n",
    "    initialize_error_logs_table()\n",
    "\n",
    "    # Define maxprice and RootPropertyType lists\n",
    "    maxprice_list = [500, 1000, 1500, 2000]\n",
    "    RootPropertyType_list = [1, 3]  # Example RootPropertyTypes\n",
    "\n",
    "    # Process PLZs and scrape listings\n",
    "    process_plz_with_retries(maxprice_list, RootPropertyType_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Master Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Database file\n",
    "DATABASE_FILE = \"locations.db\"\n",
    "\n",
    "def create_master_table():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop existing master table\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS master_table\")\n",
    "    \n",
    "    # Create the master table with dynamic listings columns\n",
    "    cursor.execute(\"\"\"\n",
    "        CREATE TABLE master_table AS\n",
    "        SELECT \n",
    "            cl.name_part_1 AS from_station,\n",
    "            cl.coordinate_x,\n",
    "            cl.coordinate_y,\n",
    "            cl.plz,\n",
    "            tr.to_station,\n",
    "            tr.min_duration,\n",
    "            tr.max_duration,\n",
    "            (tr.min_duration + tr.max_duration) / 2 AS duration,  -- Calculating average duration\n",
    "            tr.transfers,\n",
    "            tr.connections,\n",
    "            -- Example dynamic listings for specific maxprice and RootPropertyType\n",
    "            MAX(CASE WHEN l.maxprice = 500 AND l.RootPropertyType = 1 THEN l.listing_count ELSE 0 END) AS listings_500_type_1,\n",
    "            MAX(CASE WHEN l.maxprice = 500 AND l.RootPropertyType = 3 THEN l.listing_count ELSE 0 END) AS listings_500_type_3,                   \n",
    "            MAX(CASE WHEN l.maxprice = 1000 AND l.RootPropertyType = 1 THEN l.listing_count ELSE 0 END) AS listings_1000_type_1,\n",
    "            MAX(CASE WHEN l.maxprice = 1000 AND l.RootPropertyType = 3 THEN l.listing_count ELSE 0 END) AS listings_1000_type_3,\n",
    "            MAX(CASE WHEN l.maxprice = 1500 AND l.RootPropertyType = 1 THEN l.listing_count ELSE 0 END) AS listings_1500_type_1,\n",
    "            MAX(CASE WHEN l.maxprice = 1500 AND l.RootPropertyType = 3 THEN l.listing_count ELSE 0 END) AS listings_1500_type_3,\n",
    "            MAX(CASE WHEN l.maxprice = 2000 AND l.RootPropertyType = 1 THEN l.listing_count ELSE 0 END) AS listings_2000_type_1,\n",
    "            MAX(CASE WHEN l.maxprice = 2000 AND l.RootPropertyType = 3 THEN l.listing_count ELSE 0 END) AS listings_2000_type_3,\n",
    "            CASE\n",
    "                WHEN tr.to_station LIKE '%ETH%' THEN 'ETH'\n",
    "                WHEN tr.to_station LIKE '%UZH%' THEN 'UZH'\n",
    "                WHEN tr.to_station LIKE '%PHZH%' THEN 'PHZH'\n",
    "                WHEN tr.to_station LIKE '%Winter%' THEN 'ZHAW'  \n",
    "                ELSE 'Other'\n",
    "            END AS university\n",
    "        FROM combined_locations_with_plz AS cl\n",
    "        LEFT JOIN transport_results_2 AS tr  -- Updated table name\n",
    "        ON cl.name_part_1 = tr.from_station\n",
    "        LEFT JOIN listings AS l\n",
    "        ON cl.plz = l.plz\n",
    "        GROUP BY cl.name_part_1, cl.coordinate_x, cl.coordinate_y, cl.plz, tr.to_station, tr.min_duration, tr.max_duration, tr.transfers, tr.connections\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Master table has been created, including dynamic listings data.\")\n",
    "\n",
    "def fetch_master_table():\n",
    "    conn = sqlite3.connect(DATABASE_FILE)\n",
    "    query = \"SELECT * FROM master_table\"\n",
    "    master_df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return master_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the master table\n",
    "    create_master_table()\n",
    "    \n",
    "    # Fetch and display the master table\n",
    "    master_table = fetch_master_table()\n",
    "    print(master_table.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
